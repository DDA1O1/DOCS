---
title: Maths
---

## Statistics

Statistics is a branch of mathematics dealing with data collection, analysis, interpretation, and presentation. It provides tools for making informed decisions based on data.

### Key Concepts

- **Descriptive Statistics**: Summarizes data using measures like mean, median, mode, and standard deviation.
- **Inferential Statistics**: Makes predictions or inferences about a population based on a sample of data.

#### Population vs Sample

- **Population** 📊
  - Complete dataset
  - Example: All students in a university
  - N = Total size

- **Sample** 🔍
  - Subset of population
  - Example: 100 randomly selected students
  - n = Sample size

> **Key Point**: Sample should be representative of the population


### Measures of Central Tendency

- Measures of Central Tendency are used to describe the central or typical value of a dataset.

- **Mean**: 
  - Average of all values in a dataset
  - Sample Formula: x̄ = (∑x) / n
  - Population Formula: μ = (∑x) / N
  - Example: For [2,4,6,8], Mean = (2+4+6+8)/4 = 5
  - Use when:
    - Data is normally distributed
    - Need a value affected by all data points
  - Limitations:
    - Sensitive to outliers
    - May not represent central value in skewed data

- **Median**:
  - Middle value when data is ordered
  - Formula: 
    - Odd n: Value at position (n+1)/2
    - Even n: Average of values at n/2 and (n/2)+1
  - Example: For [1,3,5,7,9], Median = 5
  - Use when:
    - Data has outliers
    - Distribution is skewed
  - Limitations:
    - Not influenced by all values
    - Changes less smoothly than mean

- **Mode**:
  - Most frequently occurring value
  - Formula: Value with highest frequency
  - Example: For [1,2,2,3,4], Mode = 2
  - Use when:
    - Working with categorical data
    - Need most common value
  - Limitations:
    - Can have multiple modes
    - May not exist if all values occur once

### Measures of Dispersion

Measures of dispersion describe how spread out data points are from the center. They're crucial for:
- Understanding data variability
- Assessing data reliability
- Comparing datasets

#### Common Measures

- **Variance** (σ²)
  - Average squared deviation from mean
  - Formula: σ² = Σ(x - μ)²/n
  - Use when:
    - Detailed spread analysis needed
    - Computing statistical tests
  - Limitation: Units are squared

- **Standard Deviation** (σ)
  - Square root of variance
  - Formula: σ = √(Σ(x - μ)²/n)
  - Use when:
    - Need spread in original units
    - Analyzing normal distributions
    - Building ML models

#### Variance

Variance measures the average squared distance of data points from their mean, indicating data spread and variability.

##### Key Points

- **Definition**: Average of squared deviations from mean
- **Population Formula**: σ² = Σ(x - μ)²/N
- **Sample Formula**: s² = Σ(x - x̄)²/(n-1)

##### Why Use n-1 in Sample Variance?

The use of (n-1) instead of n in sample variance is called **Bessel's Correction**. Here's why it matters:

1. **Degrees of Freedom**
   - When calculating sample variance, we lose one degree of freedom
   - This happens because we already used one piece of information (sample mean)
   - n-1 accounts for this lost degree of freedom

2. **Bias Correction**
   - Sample variance with n tends to underestimate population variance
   - Using n-1 makes the estimator unbiased
   - Formula: s² = Σ(x - x̄)²/(n-1)

3. **Practical Impact**
   - More noticeable in small samples
   - Example:
     - n=5: 20% difference
     - n=100: 1% difference
   - Critical for accurate statistical inference

##### Real-World Example of n-1 in Sample Variance

Imagine a battery manufacturing plant:

**Population Data:**
- All 1000 batteries produced in a day
- True population mean (μ) = 1.5V
- Population readings vary between 1.3V to 1.7V
- True population variance = 0.024V²

**Sample Test:**
- We test only 5 batteries: [1.3V, 1.4V, 1.5V, 1.6V, 1.7V]
- Sample mean (x̄) = 1.5V

**Variance Calculations:**
```python
# Using n (biased)
Variance_n = Σ(x - x̄)²/5
= [(1.3-1.5)² + (1.4-1.5)² + (1.5-1.5)² + (1.6-1.5)² + (1.7-1.5)²]/5
= 0.02V² # Underestimates true variance (0.024V²)

# Using n-1 (unbiased)
Variance_n1 = Σ(x - x̄)²/4
= 0.025V² # Closer to true variance (0.024V²)
```

**Why This Matters:**
- Using n: 0.02V² (off by 0.004V²)
- Using n-1: 0.025V² (off by 0.001V²)
- n-1 gives estimate closer to true population variance

This example shows how n-1 provides a better estimate of the true population variance. Note that in real situations, we usually don't know the true population variance - that's why we need good estimation methods.

> **Key Point**: Use n-1 for sample variance to get an unbiased estimate of population variance

```python
# Sample of 100 people's weights
mean = 70kg
variance = 25kg²  # Standard deviation ≈ 5kg

# Practical Use
size_range = mean ± (2 × √variance)
# = 70 ± 10kg
# = 60kg to 80kg

# Example with weight data
mean = 70kg
std_dev = 5kg

# Coverage ranges
1σ range = 70 ± 5kg   = 65-75kg   (covers 68%)
2σ range = 70 ± 10kg  = 60-80kg   (covers 95%)  # Most commonly used
3σ range = 70 ± 15kg  = 55-85kg   (covers 99.7%)
```

### Variables

Variables are characteristics that can be measured or categorized. They come in different types:

#### 1. Qualitative (Categorical) Variables
- **Nominal**
  - Categories with no order
  - Example: Colors (red, blue), Gender (male, female)
  - Analysis: Mode, frequency

- **Ordinal**
  - Categories with order
  - Example: Education (high school, bachelor's, master's)
  - Analysis: Median, percentiles

#### 2. Quantitative (Numerical) Variables
- **Discrete**
  - Countable values
  - Example: Number of children, Test score
  - Analysis: Mean, standard deviation

- **Continuous**
  - Infinite possible values
  - Example: Height, Weight, Time
  - Analysis: Mean, standard deviation, correlation

#### Variable Relationships
- **Independent Variable (X)**
  - Manipulated/controlled variable
  - Example: Study hours

- **Dependent Variable (Y)**
  - Outcome variable
  - Example: Test score

> **Note**: Variable type determines which statistical methods to use


### Random Variables

A random variable is a function that assigns numerical values to outcomes of a random experiment.

#### Types of Random Variables

1. **Discrete Random Variables**
   - Takes countable/finite values
   - Examples:
     - Number of heads in coin flips
     - Count of defective items
   - Properties:
     - Probability Mass Function (PMF)
     - Cumulative Distribution Function (CDF)

2. **Continuous Random Variables**
   - Takes infinite possible values
   - Examples:
     - Height of a person
     - Time to complete a task
   - Properties:
     - Probability Density Function (PDF)
     - Cumulative Distribution Function (CDF)

### Histogram

A histogram is a graphical representation of the distribution of data. It shows the frequency of each data point in a dataset.

#### Key Points

- **Definition**: Bar chart showing frequency of data points
- **Purpose**: Visualize data distribution
- **Components**:
  - X-axis: Data range (bins)
  - Y-axis: Frequency (count or percentage)
  - Bars: Represents frequency of data in each bin

#### Examples

```python
import numpy as np
import matplotlib.pyplot as plt

data = [1, 2, 2, 3, 3, 3, 4, 4, 5]
plt.figure(figsize=(8, 4))
plt.hist(data, bins=5, edgecolor='black')
plt.title('Simple Histogram')
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.show()
```

![Histogram Output](/images/histogram.png)

### Percentiles and Quartiles

Percentiles and quartiles are measures that divide a dataset into equal portions.

#### Percentiles
- Divides data into 100 equal parts
- Pth percentile: Value below which P% of observations fall
- Common uses:
  - 50th percentile = median
  - Used in standardized testing (e.g., "90th percentile")

#### Quartiles
- Divides data into 4 equal parts
- Q1 (25th percentile): First quartile
- Q2 (50th percentile): Median
- Q3 (75th percentile): Third quartile
- IQR (Interquartile Range) = Q3 - Q1

#### Example

```python
import numpy as np

data = [2, 4, 6, 8, 10, 12, 14, 16]

# Quartiles
Q1 = np.percentile(data, 25)  # = 5
Q2 = np.percentile(data, 50)  # = 9
Q3 = np.percentile(data, 75)  # = 13
IQR = Q3 - Q1                 # = 8

# Any percentile
p90 = np.percentile(data, 90) # 14.6
```

### 5 Number Summary

The 5 number summary is a quick way to describe the distribution of a dataset. It consists of the minimum, first quartile, median, third quartile, and maximum.

- Minimum: Smallest value in the dataset
- First Quartile (Q1): 25th percentile
- Median: 50th percentile
- Third Quartile (Q3): 75th percentile
- Maximum: Largest value in the dataset

#### Example

```python
import numpy as np

data = [2, 4, 6, 8, 10, 12, 14, 16]

# 5 Number Summary
summary = np.percentile(data, [0, 25, 50, 75, 100])
print(summary)  # [2.  5.  9.  13. 16.]
```
Outliers are values that fall outside the range of the 5 number summary.

- Lower Outlier: Below (Q1 - 1.5 * IQR)
- Upper Outlier: Above (Q3 + 1.5 * IQR)
- Interquartile Range (IQR) = Q3 - Q1

### Example

```python
import numpy as np

# Sample dataset
data = [1, 2, 2, 3, 4, 10, 20, 25, 30]

# Calculate 5 number summary
min_val = np.min(data)
q1 = np.percentile(data, 25)
median = np.percentile(data, 50)
q3 = np.percentile(data, 75)
max_val = np.max(data)

# Calculate IQR and outlier bounds
iqr = q3 - q1
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr

# Find outliers
outliers = [x for x in data if x < lower_bound or x > upper_bound]

print(f"5 Number Summary:")
print(f"Min: {min_val}")
print(f"Q1: {q1}")
print(f"Median: {median}")
print(f"Q3: {q3}")
print(f"Max: {max_val}")
print(f"Outliers: {outliers}")
```
### Covariance and Correlation

#### Covariance

Covariance measures how two variables change together. It indicates the direction of the linear relationship between variables.

#### Formula
- Population Covariance: σxy = Σ((x - μx)(y - μy))/N
- Sample Covariance: sxy = Σ((x - x̄)(y - ȳ))/(n-1)

#### Interpretation
- Positive covariance: Variables tend to move in same direction
- Negative covariance: Variables tend to move in opposite directions
- Zero covariance: No linear relationship

#### Example

```python
import numpy as np

# Height (cm) and Weight (kg) data
height = np.array([170, 175, 160, 180, 165, 172])
weight = np.array([65, 70, 55, 80, 60, 68])

# Calculate means
height_mean = np.mean(height)
weight_mean = np.mean(weight)

# Calculate covariance manually
n = len(height)
covariance = sum((height - height_mean) * (weight - weight_mean)) / (n-1)

# Using NumPy
cov_matrix = np.cov(height, weight)
covariance_np = cov_matrix[0,1]

print(f"Manual Covariance: {covariance:.2f}")
print(f"NumPy Covariance: {covariance_np:.2f}")

# Output:
# Manual Covariance: 58.17
# NumPy Covariance: 58.17
```

#### Key Points:
1. Covariance range: -∞ to +∞
2. Scale-dependent (affected by units)
3. Used in:
   - Principal Component Analysis
   - Portfolio optimization
   - Feature selection in ML

#### Limitations:
1. Not standardized (hard to compare)
2. Units are product of both variables

For standardized measurement, use correlation instead.

#### Correlation

Correlation measures the strength and direction of the linear relationship between two variables. Unlike covariance, it's standardized between -1 and +1.

#### Pearson Correlation Coefficient
The most common correlation measure is Pearson's r:

#### Formula
- Population Correlation: ρxy = Σ((x - μx)(y - μy))/(σxσy)
- Sample Correlation: r = Σ((x - x̄)(y - ȳ))/√[Σ(x - x̄)²Σ(y - ȳ)²] = Cov(x,y)/(σxσy)

#### Interpretation
- r = 1: Perfect positive correlation
- r = -1: Perfect negative correlation  
- r = 0: No linear correlation
- |r| > 0.7: Strong correlation
- 0.3 < |r| < 0.7: Moderate correlation
- |r| < 0.3: Weak correlation

#### Example

```python
import numpy as np
import matplotlib.pyplot as plt

# Sample data
x = np.array([1, 2, 3, 4, 5])
y1 = np.array([2, 4, 6, 8, 10])  # Perfect positive
y2 = np.array([10, 8, 6, 4, 2])  # Perfect negative
y3 = np.array([2, 5, 4, 5, 3])   # Weak correlation

def plot_correlation(x, y, title):
    plt.scatter(x, y)
    plt.title(f'{title} (r = {np.corrcoef(x,y)[0,1]:.2f})')
    plt.xlabel('X')
    plt.ylabel('Y')

# Create subplots
plt.figure(figsize=(15, 5))

plt.subplot(131)
plot_correlation(x, y1, "Perfect Positive")

plt.subplot(132)
plot_correlation(x, y2, "Perfect Negative")

plt.subplot(133)
plot_correlation(x, y3, "Weak")

plt.tight_layout()
plt.show()

# Calculate correlations
print(f"Positive correlation: {np.corrcoef(x,y1)[0,1]:.2f}")
print(f"Negative correlation: {np.corrcoef(x,y2)[0,1]:.2f}")
print(f"Weak correlation: {np.corrcoef(x,y3)[0,1]:.2f}")
```

#### Key Properties
1. Scale-independent (standardized)
2. Always between -1 and +1
3. No units
4. Symmetric: corr(x,y) = corr(y,x)

#### Common Uses
- Feature selection in ML
- Financial portfolio analysis
- Scientific research
- Quality control

#### Limitations
1. Only measures linear relationships
2. Sensitive to outliers
3. Correlation ≠ causation
4. Requires numeric data

#### Real-World Example
```python
import pandas as pd

# Student data
data = {
    'study_hours': [2, 3, 3, 4, 4, 5, 5, 6],
    'test_score': [65, 70, 75, 80, 85, 85, 90, 95]
}

df = pd.DataFrame(data)

# Calculate correlation
correlation = df['study_hours'].corr(df['test_score'])

print(f"Correlation between study hours and test scores: {correlation:.2f}")
# Output: Correlation between study hours and test scores: 0.97
```

#### Important Notes
1. High correlation doesn't imply causation
2. Always visualize data - don't rely solely on correlation coefficient
3. Consider non-linear relationships
4. Check for outliers that might affect correlation
