---
title: Maths
---

## Statistics

Statistics is a branch of mathematics dealing with data collection, analysis, interpretation, and presentation. It provides tools for making informed decisions based on data.

### Key Concepts

- **Descriptive Statistics**: Summarizes data using measures like mean, median, mode, and standard deviation.
- **Inferential Statistics**: Makes predictions or inferences about a population based on a sample of data.

#### Population vs Sample

- **Population** 📊
  - Complete dataset
  - Example: All students in a university
  - N = Total size

- **Sample** 🔍
  - Subset of population
  - Example: 100 randomly selected students
  - n = Sample size

> **Key Point**: Sample should be representative of the population


### Measures of Central Tendency

- Measures of Central Tendency are used to describe the central or typical value of a dataset.

- **Mean**: 
  - Average of all values in a dataset
  - Sample Formula: x̄ = (∑x) / n
  - Population Formula: μ = (∑x) / N
  - Example: For [2,4,6,8], Mean = (2+4+6+8)/4 = 5
  - Use when:
    - Data is normally distributed
    - Need a value affected by all data points
  - Limitations:
    - Sensitive to outliers
    - May not represent central value in skewed data

- **Median**:
  - Middle value when data is ordered
  - Formula: 
    - Odd n: Value at position (n+1)/2
    - Even n: Average of values at n/2 and (n/2)+1
  - Example: For [1,3,5,7,9], Median = 5
  - Use when:
    - Data has outliers
    - Distribution is skewed
  - Limitations:
    - Not influenced by all values
    - Changes less smoothly than mean

- **Mode**:
  - Most frequently occurring value
  - Formula: Value with highest frequency
  - Example: For [1,2,2,3,4], Mode = 2
  - Use when:
    - Working with categorical data
    - Need most common value
  - Limitations:
    - Can have multiple modes
    - May not exist if all values occur once

### Measures of Dispersion

Measures of dispersion describe how spread out data points are from the center. They're crucial for:
- Understanding data variability
- Assessing data reliability
- Comparing datasets

#### Common Measures

- **Variance** (σ²)
  - Average squared deviation from mean
  - Formula: σ² = Σ(x - μ)²/n
  - Use when:
    - Detailed spread analysis needed
    - Computing statistical tests
  - Limitation: Units are squared

- **Standard Deviation** (σ)
  - Square root of variance
  - Formula: σ = √(Σ(x - μ)²/n)
  - Use when:
    - Need spread in original units
    - Analyzing normal distributions
    - Building ML models

#### Variance

Variance measures the average squared distance of data points from their mean, indicating data spread and variability.

##### Key Points

- **Definition**: Average of squared deviations from mean
- **Population Formula**: σ² = Σ(x - μ)²/N
- **Sample Formula**: s² = Σ(x - x̄)²/(n-1)

##### Why Use n-1 in Sample Variance?

The use of (n-1) instead of n in sample variance is called **Bessel's Correction**. Here's why it matters:

1. **Degrees of Freedom**
   - When calculating sample variance, we lose one degree of freedom
   - This happens because we already used one piece of information (sample mean)
   - n-1 accounts for this lost degree of freedom

2. **Bias Correction**
   - Sample variance with n tends to underestimate population variance
   - Using n-1 makes the estimator unbiased
   - Formula: s² = Σ(x - x̄)²/(n-1)

3. **Practical Impact**
   - More noticeable in small samples
   - Example:
     - n=5: 20% difference
     - n=100: 1% difference
   - Critical for accurate statistical inference

##### Real-World Example of n-1 in Sample Variance

Imagine a battery manufacturing plant:

**Population Data:**
- All 1000 batteries produced in a day
- True population mean (μ) = 1.5V
- Population readings vary between 1.3V to 1.7V
- True population variance = 0.024V²

**Sample Test:**
- We test only 5 batteries: [1.3V, 1.4V, 1.5V, 1.6V, 1.7V]
- Sample mean (x̄) = 1.5V

**Variance Calculations:**
```python
# Using n (biased)
Variance_n = Σ(x - x̄)²/5
= [(1.3-1.5)² + (1.4-1.5)² + (1.5-1.5)² + (1.6-1.5)² + (1.7-1.5)²]/5
= 0.02V² # Underestimates true variance (0.024V²)

# Using n-1 (unbiased)
Variance_n1 = Σ(x - x̄)²/4
= 0.025V² # Closer to true variance (0.024V²)
```

**Why This Matters:**
- Using n: 0.02V² (off by 0.004V²)
- Using n-1: 0.025V² (off by 0.001V²)
- n-1 gives estimate closer to true population variance

This example shows how n-1 provides a better estimate of the true population variance. Note that in real situations, we usually don't know the true population variance - that's why we need good estimation methods.

> **Key Point**: Use n-1 for sample variance to get an unbiased estimate of population variance

```python
# Sample of 100 people's weights
mean = 70kg
variance = 25kg²  # Standard deviation ≈ 5kg

# Practical Use
size_range = mean ± (2 × √variance)
# = 70 ± 10kg
# = 60kg to 80kg

# Example with weight data
mean = 70kg
std_dev = 5kg

# Coverage ranges
1σ range = 70 ± 5kg   = 65-75kg   (covers 68%)
2σ range = 70 ± 10kg  = 60-80kg   (covers 95%)  # Most commonly used
3σ range = 70 ± 15kg  = 55-85kg   (covers 99.7%)
```

### Variables

Variables are characteristics that can be measured or categorized. They come in different types:

#### 1. Qualitative (Categorical) Variables
- **Nominal**
  - Categories with no order
  - Example: Colors (red, blue), Gender (male, female)
  - Analysis: Mode, frequency

- **Ordinal**
  - Categories with order
  - Example: Education (high school, bachelor's, master's)
  - Analysis: Median, percentiles

#### 2. Quantitative (Numerical) Variables
- **Discrete**
  - Countable values
  - Example: Number of children, Test score
  - Analysis: Mean, standard deviation

- **Continuous**
  - Infinite possible values
  - Example: Height, Weight, Time
  - Analysis: Mean, standard deviation, correlation

#### Variable Relationships
- **Independent Variable (X)**
  - Manipulated/controlled variable
  - Example: Study hours

- **Dependent Variable (Y)**
  - Outcome variable
  - Example: Test score

> **Note**: Variable type determines which statistical methods to use


### Random Variables

A random variable is a function that assigns numerical values to outcomes of a random experiment.

#### Types of Random Variables

1. **Discrete Random Variables**
   - Takes countable/finite values
   - Examples:
     - Number of heads in coin flips
     - Count of defective items
   - Properties:
     - Probability Mass Function (PMF)
     - Cumulative Distribution Function (CDF)

2. **Continuous Random Variables**
   - Takes infinite possible values
   - Examples:
     - Height of a person
     - Time to complete a task
   - Properties:
     - Probability Density Function (PDF)
     - Cumulative Distribution Function (CDF)

### Histogram

A histogram is a graphical representation of the distribution of data. It shows the frequency of each data point in a dataset.

#### Key Points

- **Definition**: Bar chart showing frequency of data points
- **Purpose**: Visualize data distribution
- **Components**:
  - X-axis: Data range (bins)
  - Y-axis: Frequency (count or percentage)
  - Bars: Represents frequency of data in each bin

#### Examples

```python
import numpy as np
import matplotlib.pyplot as plt

data = [1, 2, 2, 3, 3, 3, 4, 4, 5]
plt.figure(figsize=(8, 4))
plt.hist(data, bins=5, edgecolor='black')
plt.title('Simple Histogram')
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.show()
```

![Histogram Output](/images/histogram.png)

### Percentiles and Quartiles

Percentiles and quartiles are measures that divide a dataset into equal portions.

#### Percentiles
- Divides data into 100 equal parts
- Pth percentile: Value below which P% of observations fall
- Common uses:
  - 50th percentile = median
  - Used in standardized testing (e.g., "90th percentile")

#### Quartiles
- Divides data into 4 equal parts
- Q1 (25th percentile): First quartile
- Q2 (50th percentile): Median
- Q3 (75th percentile): Third quartile
- IQR (Interquartile Range) = Q3 - Q1

#### Example

```python
import numpy as np

data = [2, 4, 6, 8, 10, 12, 14, 16]

# Quartiles
Q1 = np.percentile(data, 25)  # = 5
Q2 = np.percentile(data, 50)  # = 9
Q3 = np.percentile(data, 75)  # = 13
IQR = Q3 - Q1                 # = 8

# Any percentile
p90 = np.percentile(data, 90) # 14.6
```

### 5 Number Summary

The 5 number summary is a quick way to describe the distribution of a dataset. It consists of the minimum, first quartile, median, third quartile, and maximum.

- Minimum: Smallest value in the dataset
- First Quartile (Q1): 25th percentile
- Median: 50th percentile
- Third Quartile (Q3): 75th percentile
- Maximum: Largest value in the dataset

#### Example

```python
import numpy as np

data = [2, 4, 6, 8, 10, 12, 14, 16]

# 5 Number Summary
summary = np.percentile(data, [0, 25, 50, 75, 100])
print(summary)  # [2.  5.  9.  13. 16.]
```
Outliers are values that fall outside the range of the 5 number summary.

- Lower Outlier: Below (Q1 - 1.5 * IQR)
- Upper Outlier: Above (Q3 + 1.5 * IQR)
- Interquartile Range (IQR) = Q3 - Q1

### Example

```python
import numpy as np

# Sample dataset
data = [1, 2, 2, 3, 4, 10, 20, 25, 30]

# Calculate 5 number summary
min_val = np.min(data)
q1 = np.percentile(data, 25)
median = np.percentile(data, 50)
q3 = np.percentile(data, 75)
max_val = np.max(data)

# Calculate IQR and outlier bounds
iqr = q3 - q1
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr

# Find outliers
outliers = [x for x in data if x < lower_bound or x > upper_bound]

print(f"5 Number Summary:")
print(f"Min: {min_val}")
print(f"Q1: {q1}")
print(f"Median: {median}")
print(f"Q3: {q3}")
print(f"Max: {max_val}")
print(f"Outliers: {outliers}")
```
### Covariance and Correlation

#### Covariance

Covariance measures how two variables change together. It indicates the direction of the linear relationship between variables.

#### Formula
- Population Covariance: σxy = Σ((x - μx)(y - μy))/N
- Sample Covariance: sxy = Σ((x - x̄)(y - ȳ))/(n-1)

#### Interpretation
- Positive covariance: Variables tend to move in same direction
- Negative covariance: Variables tend to move in opposite directions
- Zero covariance: No linear relationship

#### Example

```python
import numpy as np

# Height (cm) and Weight (kg) data
height = np.array([170, 175, 160, 180, 165, 172])
weight = np.array([65, 70, 55, 80, 60, 68])

# Calculate means
height_mean = np.mean(height)
weight_mean = np.mean(weight)

# Calculate covariance manually
n = len(height)
covariance = sum((height - height_mean) * (weight - weight_mean)) / (n-1)

# Using NumPy
cov_matrix = np.cov(height, weight)
covariance_np = cov_matrix[0,1]

print(f"Manual Covariance: {covariance:.2f}")
print(f"NumPy Covariance: {covariance_np:.2f}")

# Output:
# Manual Covariance: 58.17
# NumPy Covariance: 58.17
```

#### Key Points:
1. Covariance range: -∞ to +∞
2. Scale-dependent (affected by units)
3. Used in:
   - Principal Component Analysis
   - Portfolio optimization
   - Feature selection in ML

#### Limitations:
1. Not standardized (hard to compare)
2. Units are product of both variables

For standardized measurement, use correlation instead.

#### Correlation

Correlation measures the strength and direction of the linear relationship between two variables. Unlike covariance, it's standardized between -1 and +1.

#### Pearson Correlation Coefficient
The most common correlation measure is Pearson's r:

#### Formula
- Population Correlation: ρxy = Σ((x - μx)(y - μy))/(σxσy)
- Sample Correlation: r = Σ((x - x̄)(y - ȳ))/√[Σ(x - x̄)²Σ(y - ȳ)²] = Cov(x,y)/(σxσy)

#### Interpretation
- r = 1: Perfect positive correlation
- r = -1: Perfect negative correlation  
- r = 0: No linear correlation
- |r| > 0.7: Strong correlation
- 0.3 < |r| < 0.7: Moderate correlation
- |r| < 0.3: Weak correlation

#### Example

```python
import numpy as np
import matplotlib.pyplot as plt

# Sample data
x = np.array([1, 2, 3, 4, 5])
y1 = np.array([2, 4, 6, 8, 10])  # Perfect positive
y2 = np.array([10, 8, 6, 4, 2])  # Perfect negative
y3 = np.array([2, 5, 4, 5, 3])   # Weak correlation

def plot_correlation(x, y, title):
    plt.scatter(x, y)
    plt.title(f'{title} (r = {np.corrcoef(x,y)[0,1]:.2f})')
    plt.xlabel('X')
    plt.ylabel('Y')

# Create subplots
plt.figure(figsize=(15, 5))

plt.subplot(131)
plot_correlation(x, y1, "Perfect Positive")

plt.subplot(132)
plot_correlation(x, y2, "Perfect Negative")

plt.subplot(133)
plot_correlation(x, y3, "Weak")

plt.tight_layout()
plt.show()

# Calculate correlations
print(f"Positive correlation: {np.corrcoef(x,y1)[0,1]:.2f}") # 1.00
print(f"Negative correlation: {np.corrcoef(x,y2)[0,1]:.2f}") # -1.00
print(f"Weak correlation: {np.corrcoef(x,y3)[0,1]:.2f}") # 0.24
```
![Correlation](/images/Pearson_Correlation.png)

#### Key Properties
1. Scale-independent (standardized)
2. Always between -1 and +1
3. No units
4. Symmetric: corr(x,y) = corr(y,x)

#### Common Uses
- Feature selection in ML
- Financial portfolio analysis
- Scientific research
- Quality control

#### Limitations
1. Only measures linear relationships
2. Sensitive to outliers
3. Correlation ≠ causation
4. Requires numeric data

#### Real-World Example
```python
import pandas as pd

# Student data
data = {
    'study_hours': [2, 3, 3, 4, 4, 5, 5, 6],
    'test_score': [65, 70, 75, 80, 85, 85, 90, 95]
}

df = pd.DataFrame(data)

# Calculate correlation
correlation = df['study_hours'].corr(df['test_score'])

print(f"Correlation between study hours and test scores: {correlation:.2f}")
# Output: Correlation between study hours and test scores: 0.97
```

#### Important Notes
1. High correlation doesn't imply causation
2. Always visualize data - don't rely solely on correlation coefficient
3. Consider non-linear relationships
4. Check for outliers that might affect correlation

### Spearman Correlation

Spearman correlation measures monotonic relationships between variables (whether they move in the same direction, regardless of the rate of change).

#### Formula

1. First convert values to ranks
   - rank(x): Rank values of first variable 
   - rank(y): Rank values of second variable

2. Calculate differences
   d = rank(x) - rank(y)

3. Square the differences
   d² = (rank(x) - rank(y))²

4. Sum all squared differences
   Σd² = sum of all d²

5. Final formula:
   ρ = 1 - (6 * Σd²)/(n(n² - 1))

Example with numbers:
x = [1,2,3]
y = [2,1,3]

ranks_x = [1,2,3]
ranks_y = [2,1,3]

d = [1-2, 2-1, 3-3] = [-1,1,0]
d² = [1,1,0]
Σd² = 2
n = 3

ρ = 1 - (6 * 2)/(3(9-1))
  = 1 - 12/24
  = 0.5

#### Example

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Online Store Example
# X: Product Price
# Y: Number of Sales
price = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90, 100])
sales = np.array([100, 90, 80, 65, 45, 30, 20, 15, 12, 10])

# Calculate Correlations
pearson = stats.pearsonr(price, sales)[0]
spearman = stats.spearmanr(price, sales)[0]

# Plotting
plt.figure(figsize=(10, 6))
plt.scatter(price, sales)
plt.title('Price vs Sales: Non-linear Relationship')
plt.xlabel('Price ($)')
plt.ylabel('Number of Sales')

# Add correlation values
plt.text(20, 30, f'Pearson r: {pearson:.2f}')
plt.text(20, 20, f'Spearman r: {spearman:.2f}')
plt.grid(True)
plt.show()

print(f"Pearson Correlation: {pearson:.2f}")   # -0.89
print(f"Spearman Correlation: {spearman:.2f}") # -1.00
```
![Spearman Correlation](/images/Spearman_Correlation.png)

## Probability

Probability is a measure of the likelihood of an event occurring. It's a fundamental concept in statistics and data science.

#### Additive Rule

#### Mutually Exclusive Events
Events that cannot occur at the same time.

Formula:
P(A or B) = P(A) + P(B)

Example:
- Rolling a die:
  - P(getting 1 or 2) = P(1) + P(2) = 1/6 + 1/6 = 1/3
  - Events are mutually exclusive since you can't roll 1 and 2 simultaneously

#### Non-Mutually Exclusive Events 
Events that can occur at the same time.

Formula:
P(A or B) = P(A) + P(B) - P(A and B)

Example:
- Drawing a card:
  - P(getting King or Heart) = P(King) + P(Heart) - P(King of Heart)
  - = 4/52 + 13/52 - 1/52 = 16/52
  - Events overlap since King of Hearts is possible

### Multiplicative Rule

The multiplicative rule calculates probability of multiple events occurring together.

#### Independent Events
Events where occurrence of one doesn't affect the other.

Formula:
P(A and B) = P(A) × P(B)

Example:
- Flipping a coin twice:
  - P(2 heads) = P(head1) × P(head2) = 1/2 × 1/2 = 1/4

#### Dependent Events
Events where occurrence of one affects the other.

Formula:
P(A and B) = P(A) × P(B|A)

Example:
- Drawing 2 cards without replacement:
  - P(2 aces) = P(ace1) × P(ace2|ace1) 
  - = 4/52 × 3/51 = 1/221

### Relationship between PMF, PDF, and CDF

#### 1. PMF (Probability Mass Function)

A PMF describes the probability distribution of a discrete random variable.

#### Definition
- Maps each value of discrete random variable to its probability
- P(X = x) gives probability of X taking value x
- Sum of all probabilities must equal 1

#### Properties
- 0 ≤ P(X = x) ≤ 1
- ∑P(X = x) = 1
- Only for discrete variables

#### Real-World Example: Dice Roll Game

```python
import numpy as np
import matplotlib.pyplot as plt

# Fair die PMF
outcomes = np.array([1, 2, 3, 4, 5, 6])
probabilities = np.array([1/6] * 6)

# Loaded die PMF (favors 6)
loaded_prob = np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.5])

# Plot
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.bar(outcomes, probabilities)
plt.title('Fair Die PMF')
plt.xlabel('Outcome')
plt.ylabel('Probability')

plt.subplot(1, 2, 2)
plt.bar(outcomes, loaded_prob)
plt.title('Loaded Die PMF')
plt.xlabel('Outcome')
plt.show()

# Calculate probability of rolling even numbers
fair_even = sum(probabilities[1::2])    # 0.5
loaded_even = sum(loaded_prob[1::2])    # 0.7

print(f"P(Even) Fair Die: {fair_even}") # 0.5
print(f"P(Even) Loaded Die: {loaded_even}") # 0.7
```
![PMF](/images/PMF.png)

PMF helps in making probability-based decisions in discrete scenarios like manufacturing defects, customer counts, or game outcomes.

#### CDF (Cumulative Distribution Function) For Discrete Variables

CDF gives the probability that a random variable X is less than or equal to a value x.

#### Formula
F(x) = P(X ≤ x) = ∑ P(X = t) for all t ≤ x

#### Example: Die Roll

```python
import numpy as np
import matplotlib.pyplot as plt

# Define probabilities
outcomes = np.arange(1, 7)  # [1,2,3,4,5,6]
fair_prob = np.ones(6) / 6  # [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]
loaded_prob = np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.5])

# Calculate CDFs
fair_cdf = np.cumsum(fair_prob)
loaded_cdf = np.cumsum(loaded_prob)

# Create plot
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))

# Plot fair die CDF
ax1.step(outcomes, fair_cdf, where='post')
ax1.set(title='Fair Die CDF', xlabel='Outcome', ylabel='Cumulative Probability')
ax1.grid(True)

# Plot loaded die CDF
ax2.step(outcomes, loaded_cdf, where='post')
ax2.set(title='Loaded Die CDF', xlabel='Outcome')
ax2.grid(True)

plt.tight_layout()
plt.show()

# Print probabilities for X ≤ 4
print(f"P(X ≤ 4) Fair Die: {fair_cdf[3]:.3f}") # 0.667
print(f"P(X ≤ 4) Loaded Die: {loaded_cdf[3]:.3f}") # 0.400
```
![Discrete CDF](/images/Discrete_CDF.png)

#### 2. PDF (Probability Density Function)

A PDF describes the probability distribution of a continuous random variable.

### Why PDF for Continuous Random Variables?

1. **Impossible to List All Values**
   - Continuous variables have infinite possible values
   - Can't assign individual probabilities like PMF

2. **Zero Individual Probability**
   - P(X = x) = 0 for any exact value
   - Example: P(height = exactly 170.000000...cm) = 0

3. **Range Probabilities**
   - PDF helps calculate probability over intervals
   - P(a ≤ X ≤ b) = ∫[a to b] f(x)dx
   - Example: P(170 ≤ height ≤ 175)

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Height Distribution in Adult Population
mean_height = 170  # cm
std_dev = 10       # cm

# Create height range
heights = np.linspace(140, 200, 100)

# Calculate PDF using normal distribution
pdf = norm.pdf(heights, mean_height, std_dev)

# Plot
plt.figure(figsize=(10, 6))
plt.plot(heights, pdf)
plt.title('Height Distribution in Adult Population')
plt.xlabel('Height (cm)')
plt.ylabel('Probability Density')

# Calculate probabilities
# Probability of height between 160-180cm
prob_160_180 = norm.cdf(180, mean_height, std_dev) - norm.cdf(160, mean_height, std_dev)
print(f"Probability of height between 160-180cm: {prob_160_180:.2%}")  # ≈ 68%

# Probability of height above 190cm
prob_above_190 = 1 - norm.cdf(190, mean_height, std_dev)
print(f"Probability of height above 190cm: {prob_above_190:.2%}")      # ≈ 2.3%
```
![Continuous PDF](/images/Continuous_PDF.png)

### Density

Density in statistics refers to how tightly packed data points or probability is in a given interval or region.

```python
import numpy as np
from scipy.stats import norm
import matplotlib.pyplot as plt

mean = 170
std = 10

# Single point density doesn't tell much
point_density = norm.pdf(170, mean, std)
print(f"Density at 170cm: {point_density:.4f}")  # 0.0399
# This number 0.0399 alone is meaningless without context

# What makes sense is comparing densities
heights = [150, 160, 170, 180, 190]
densities = [norm.pdf(h, mean, std) for h in heights]

plt.figure(figsize=(10, 6))
x = np.linspace(140, 200, 1000)
y = norm.pdf(x, mean, std)

plt.plot(x, y)

# Plot points for comparison
for h, d in zip(heights, densities):
    plt.plot(h, d, 'o', label=f'Height={h}cm\nDensity={d:.4f}')

plt.title('Height Distribution - Comparing Densities')
plt.xlabel('Height (cm)')
plt.ylabel('Density')
plt.legend()
plt.grid(True)
plt.show()

# Now we can see:
# - 170cm has highest density (most common)
# - 150cm and 190cm have low density (less common)
# - Comparison gives meaning to the numbers
```
![Density](/images/Density.png)

#### CDF (Cumulative Distribution Function) For Continuous Variables

The Cumulative Distribution Function (CDF) for a continuous random variable X, denoted as F(x), represents the probability that X takes on a value less than or equal to x.

##### Mathematical Definition
F(x) = P(X ≤ x) = ∫[from -∞ to x] f(t)dt

where f(t) is the probability density function (PDF)

##### Key Properties
1. **Bounds**
   - 0 ≤ F(x) ≤ 1 for all x
   - lim[x→-∞] F(x) = 0
   - lim[x→∞] F(x) = 1

2. **Continuity**
   - Right-continuous
   - Monotonically increasing (never decreases)

3. **Probability Calculations**
   - P(a < X ≤ b) = F(b) - F(a)
   - P(X > a) = 1 - F(a)

4. **Relationship to PDF**
   - F'(x) = f(x) (derivative of CDF is PDF)
   - F(x) is the integral of f(x)

##### Simple Example: Height Distribution in a Class

Consider heights of students in a class of 100:

**Scenario:**
- Heights range from 150cm to 190cm
- CDF tells us probability of height being less than or equal to a value

**Simple Interpretations:**
- F(160) = 0.2 means 20% of students are 160cm or shorter
- F(170) = 0.5 means 50% of students are 170cm or shorter
- F(180) = 0.9 means 90% of students are 180cm or shorter

**Practical Uses:**
- Finding median height: Where F(x) = 0.5
- Ordering uniforms: What size covers 80% of students
- Identifying unusually tall/short: Heights where F(x) < 0.1 or F(x) > 0.9

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Parameters
mean_height = 170  # mean height in cm
std_dev = 10      # standard deviation in cm
n_students = 100

# Generate student heights
np.random.seed(42)  # for reproducibility
heights = np.random.normal(mean_height, std_dev, n_students)

# Calculate empirical CDF
heights_sorted = np.sort(heights)
cumulative_prob = np.arange(1, len(heights) + 1) / len(heights)

# Plot
plt.figure(figsize=(10, 6))

# Empirical CDF
plt.plot(heights_sorted, cumulative_prob, 'b-', label='Empirical CDF')

# Add reference lines
plt.axhline(y=0.2, color='r', linestyle='--', alpha=0.5)
plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5)
plt.axhline(y=0.9, color='r', linestyle='--', alpha=0.5)

# Labels and title
plt.title('Height Distribution CDF in Class')
plt.xlabel('Height (cm)')
plt.ylabel('Cumulative Probability')
plt.grid(True)

# Find specific values
height_20 = np.percentile(heights, 20)
height_50 = np.percentile(heights, 50)
height_90 = np.percentile(heights, 90)

print(f"20th percentile (F(x) = 0.2): {height_20:.1f}cm") # 162.6cm
print(f"50th percentile (F(x) = 0.5): {height_50:.1f}cm") # 168.7cm
print(f"90th percentile (F(x) = 0.9): {height_90:.1f}cm") # 180.1cm

plt.show()
```
![Continuous CDF](/images/Continuous_CDF.png)

### Types of Probability Distributions

#### Bernoulli Distribution

The Bernoulli distribution models binary outcomes - experiments with exactly two possible results (success/failure).

##### Properties
- **Parameter**: p (probability of success)
- **Possible Values**: x ∈ {0,1}
  - x = 1 (success): probability = p
  - x = 0 (failure): probability = 1-p
- **PMF**: P(X = x) = p^x * (1-p)^(1-x)
- **Mean**: E(X) = p
- **Variance**: Var(X) = p(1-p)

##### Common Applications
- Coin flips (heads/tails)
- Quality control (defective/non-defective)
- Email (spam/not spam)
- Medical tests (positive/negative)

##### Example Implementation

```python
import numpy as np
import matplotlib.pyplot as plt

class BernoulliTrial:
    def __init__(self, p):
        self.p = p
    
    def pmf(self, x):
        return self.p if x == 1 else (1-self.p)
    
    def simulate(self, n_trials):
        return np.random.binomial(n=1, p=self.p, size=n_trials)

# Example: Biased coin (p=0.7)
b = BernoulliTrial(p=0.7)
trials = b.simulate(1000)

# Results
success_rate = np.mean(trials)
print(f"Theoretical probability: 0.7")
print(f"Observed probability: {success_rate:.3f}")

# Visualize
plt.figure(figsize=(8, 4))
plt.bar(['Failure (0)', 'Success (1)'], 
        [1-success_rate, success_rate])
plt.title('Bernoulli Distribution (p=0.7)')
plt.ylabel('Probability')
plt.ylim(0, 1)
```
![Bernoulli Distribution](/images/Bernoulli_Distribution.png)

##### Real-World Example: Email Spam Detection

```python
# Simple spam detector
class SpamDetector:
    def __init__(self, spam_probability=0.3):
        self.b = BernoulliTrial(spam_probability)
    
    def classify_email(self):
        return "Spam" if self.b.simulate(1)[0] else "Not Spam"

# Simulate email classification
detector = SpamDetector()
n_emails = 100
classifications = [detector.classify_email() for _ in range(n_emails)]

spam_ratio = classifications.count("Spam") / n_emails
print(f"Classified {spam_ratio:.1%} emails as spam")
// Classified 26.0% emails as spam
```

##### Key Points
1. **Independence**: Each trial is independent
2. **Memory-less**: Previous outcomes don't affect next trial
3. **Fixed probability**: p remains constant across trials

##### Relationship to Other Distributions
- Foundation for Binomial distribution (n Bernoulli trials)
- Special case of Binomial where n=1
- Building block for more complex probability models

#### Binomial Distribution

The Binomial distribution models the number of successes in n independent Bernoulli trials.

##### Properties
- **Parameters**: 
  - n (number of trials)
  - p (probability of success)
- **PMF**: P(X = k) = C(n,k) * p^k * (1-p)^(n-k)
- **Mean**: E(X) = np
- **Variance**: Var(X) = np(1-p)

##### Example Implementation

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import binom

class BinomialDistribution:
    def __init__(self, n, p):
        self.n = n
        self.p = p
    
    def pmf(self, k):
        return binom.pmf(k, self.n, self.p)
    
    def simulate(self, n_trials):
        return np.random.binomial(self.n, self.p, n_trials)

# Example: Rolling a fair die 10 times, counting 6s
n, p = 10, 1/6  # 10 rolls, P(6) = 1/6
b = BinomialDistribution(n, p)

# Calculate PMF for all possible values
k = np.arange(0, n+1)
probabilities = [b.pmf(ki) for ki in k]

# Plot
plt.figure(figsize=(10, 5))
plt.bar(k, probabilities)
plt.title(f'Binomial Distribution (n={n}, p={p:.2f})')
plt.xlabel('Number of Successes (k)')
plt.ylabel('Probability')
plt.grid(True, alpha=0.3)

# Expected value and variance
mean = n * p
var = n * p * (1-p)
print(f"Expected number of 6s: {mean:.2f}") # 1.67
print(f"Variance: {var:.2f}") # 1.39
```
![Binomial Distribution](/images/Binomial_Distribution.png)

##### Real-World Example: Quality Control

```python
# Manufacturing defect inspection
class QualityControl:
    def __init__(self, batch_size=20, defect_rate=0.05):
        self.binom = BinomialDistribution(batch_size, defect_rate)
    
    def inspect_batch(self):
        return self.binom.simulate(1)[0]
    
    def is_batch_acceptable(self, max_defects=2):
        defects = self.inspect_batch()
        return {
            'defects': defects,
            'acceptable': defects <= max_defects
        }

# Simulate batch inspections
qc = QualityControl()
n_batches = 1000
inspections = [qc.is_batch_acceptable() for _ in range(n_batches)]

acceptance_rate = sum(i['acceptable'] for i in inspections) / n_batches
print(f"Batch acceptance rate: {acceptance_rate:.1%}")
// Batch acceptance rate: 92.7%
```

##### Common Applications
1. Quality control in manufacturing
2. A/B testing success counts
3. Survey response modeling
4. Genetic inheritance patterns

##### Key Points
1. Sum of independent Bernoulli trials
2. Requires fixed probability p
3. Trials must be independent
4. Only whole numbers (discrete)

#### Poisson Distribution

The Poisson distribution models the number of events occurring in a fixed interval when these events happen at a constant average rate and independently of each other.

##### Properties
- **Parameter**: λ (lambda) - average number of events per interval
- **PMF**: P(X = k) = (λ^k * e^-λ) / k!
- **Mean**: E(X) = λ
- **Variance**: Var(X) = λ

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import poisson

class PoissonDistribution:
    def __init__(self, lambda_param):
        self.lambda_param = lambda_param
    
    def pmf(self, k):
        return poisson.pmf(k, self.lambda_param)
    
    def simulate(self, n_samples):
        return np.random.poisson(self.lambda_param, n_samples)

# Example: Website visits per hour (average 5 visits)
lambda_param = 5
p = PoissonDistribution(lambda_param)

# Calculate PMF for values 0 to 12
k = np.arange(0, 13)
probabilities = [p.pmf(ki) for ki in k]

# Visualization
plt.figure(figsize=(10, 6))
plt.bar(k, probabilities)
plt.title(f'Poisson Distribution (λ={lambda_param})')
plt.xlabel('Number of Events (k)')
plt.ylabel('Probability')
plt.grid(True, alpha=0.3)
```
![Poisson Distribution](/images/Poisson_Distribution.png)

##### Common Applications
1. **Customer Service**
   - Number of customers arriving per hour
   - Support tickets received per day
   - Phone calls to call center

2. **Web Traffic**
   - Page views per minute
   - Server requests per second
   - Error occurrences per day

3. **Quality Control**
   - Defects per unit area
   - Flaws per length of material
   - Errors per page

4. **Natural Phenomena**
   - Radioactive decay events
   - Mutations in DNA sequence
   - Natural disasters per year

##### Real-World Example: Server Monitoring

```python
class ServerMonitor:
    def __init__(self, avg_requests_per_minute=30):
        self.poisson = PoissonDistribution(avg_requests_per_minute)
    
    def simulate_minute(self):
        return self.poisson.simulate(1)[0]
    
    def check_load(self, threshold=50):
        requests = self.simulate_minute()
        return {
            'requests': requests,
            'overloaded': requests > threshold,
            'utilization': requests / threshold
        }

# Monitor server for an hour
monitor = ServerMonitor()
hour_data = [monitor.check_load() for _ in range(60)]

# Analysis
overloaded_minutes = sum(minute['overloaded'] for minute in hour_data)
avg_utilization = np.mean([minute['utilization'] for minute in hour_data])

print(f"Minutes overloaded: {overloaded_minutes}") // Minutes overloaded: 0
print(f"Average utilization: {avg_utilization:.1%}") // Average utilization: 56.5%
```

##### Key Characteristics

1. **Independence**
   - Events occur independently
   - Past events don't influence future events

2. **Rate Consistency**
   - Average rate (λ) remains constant
   - No systematic variation in event frequency

3. **Rare Events**
   - Individual events are rare relative to opportunities
   - Many opportunities for events to occur

4. **No Upper Limit**
   - Can theoretically take any non-negative integer value
   - Practical limits depend on λ

##### Relationship to Other Distributions

1. **Binomial Distribution**
   - Poisson is limit of binomial as n→∞, p→0, np=λ
   - Used when events are rare but opportunities numerous

2. **Exponential Distribution**
   - Time between Poisson events follows exponential distribution
   - If events are Poisson(λ), waiting times are Exp(1/λ)

##### Assumptions and Limitations

1. **Rate Stability**
   - Assumes constant average rate
   - May not fit if rate varies significantly

2. **Independence**
   - Events must be independent
   - Not suitable for contagious or clustered events

3. **No Simultaneous Events**
   - Events occur one at a time
   - May need modifications for concurrent events

4. **Memory-less Property**
   - Future events independent of past
   - May not suit events with temporal dependencies

```python
# Example: Testing Poisson assumptions
def test_rate_stability(data, window_size=10):
    """Test if event rate is stable over time"""
    windows = np.array_split(data, len(data)//window_size)
    means = [np.mean(w) for w in windows]
    return np.std(means) / np.mean(means)  # CV should be small

# Generate sample data
p = PoissonDistribution(lambda_param=5)
data = p.simulate(1000)

stability_metric = test_rate_stability(data)
print(f"Rate stability metric: {stability_metric:.3f}")
# Lower values indicate more stable rate 
// Rate stability metric: 0.138
```

#### Normal Distribution/Gaussian Distribution

The Normal (or Gaussian) distribution is a continuous probability distribution that is symmetric around its mean, showing a characteristic "bell-shaped" curve.

##### Properties
- **Parameters**: 
  - μ (mean): Center of distribution
  - σ (standard deviation): Spread of distribution
- **PDF**: f(x) = (1/(σ√(2π))) * e^(-(x-μ)²/(2σ²))
- **Mean = Median = Mode**: All equal to μ
- **68-95-99.7 Rule**:
  - 68% of data within μ ± 1σ
  - 95% of data within μ ± 2σ
  - 99.7% of data within μ ± 3σ

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

class NormalDistribution:
    def __init__(self, mu=0, sigma=1):
        self.mu = mu
        self.sigma = sigma
    
    def pdf(self, x):
        return norm.pdf(x, self.mu, self.sigma)
    
    def simulate(self, n_samples):
        return np.random.normal(self.mu, self.sigma, n_samples)

# Example: Height Distribution
mu, sigma = 170, 10  # mean=170cm, std=10cm
normal = NormalDistribution(mu, sigma)

# Generate x values and corresponding probabilities
x = np.linspace(mu - 4*sigma, mu + 4*sigma, 100)
y = normal.pdf(x)

# Plot
plt.figure(figsize=(10, 6))
plt.plot(x, y, 'b-', label='PDF')

# Add standard deviation ranges
for i, pct in [(1, 0.68), (2, 0.95), (3, 0.997)]:
    plt.fill_between(x, y, 
                     where=(x >= mu-i*sigma) & (x <= mu+i*sigma),
                     alpha=0.2,
                     label=f'±{i}σ ({pct:.1%})')

plt.title('Normal Distribution of Heights')
plt.xlabel('Height (cm)')
plt.ylabel('Probability Density')
plt.legend()
plt.grid(True)
```
![Normal Distribution](/images/Normal_Distribution.png)

##### Common Applications

1. **Physical Measurements**
   - Height, weight
   - Manufacturing dimensions
   - Measurement errors

2. **Natural Phenomena**
   - IQ scores
   - Blood pressure
   - Test scores

3. **Financial Markets**
   - Stock returns
   - Price fluctuations
   - Risk modeling

##### Real-World Example: Quality Control

```python
class ProductionLine:
    def __init__(self, target_length=100, tolerance=0.5):
        self.normal = NormalDistribution(target_length, tolerance/3)
        self.tolerance = tolerance
        
    def produce_item(self):
        length = self.normal.simulate(1)[0]
        return {
            'length': length,
            'in_spec': abs(length - self.normal.mu) <= self.tolerance
        }
    
    def analyze_batch(self, size=1000):
        batch = [self.produce_item() for _ in range(size)]
        defect_rate = 1 - sum(item['in_spec'] for item in batch) / size
        return f"Defect rate: {defect_rate:.2%}"

# Simulate production
line = ProductionLine()
print(line.analyze_batch())  # Expected ≈ 0.20% defect rate
```

##### Z-Score (Standard Score)

Z-score measures how many standard deviations away from the mean a data point is:
- Formula: z = (x - μ) / σ
- Standardizes any normal distribution to N(0,1)
- Useful for comparing values from different distributions

```python
def calculate_z_score(x, mu, sigma):
    return (x - mu) / sigma

# Example
height = 185  # cm
z = calculate_z_score(height, mu=170, sigma=10)
print(f"Z-score for {height}cm: {z:.2f}")  # 1.50
print(f"Percentile: {norm.cdf(z):.2%}")    # 93.32%
```

##### Central Limit Theorem (CLT)

The CLT states that the sampling distribution of the mean approaches a normal distribution as sample size increases, regardless of the underlying distribution:

```python
def demonstrate_clt(distribution, sample_size, n_samples):
    means = [np.mean(distribution(sample_size)) 
             for _ in range(n_samples)]
    return means

# Example with uniform distribution
uniform_samples = lambda n: np.random.uniform(0, 1, n)
sample_means = demonstrate_clt(uniform_samples, 30, 1000)

plt.figure(figsize=(10, 6))
plt.hist(sample_means, bins=30, density=True)
plt.title('Sampling Distribution of Mean (n=30)')
plt.xlabel('Sample Mean')
plt.ylabel('Density')
```
![Central Limit Theorem](/images/Central_Limit_Theorem.png)

##### Key Points

1. **Symmetry**
   - Perfectly symmetric around mean
   - Skewness = 0
   - Kurtosis = 3

2. **Standardization**
   - Any normal distribution can be standardized
   - Z-scores allow comparison across distributions

3. **Empirical Rule**
   - 68-95-99.7 rule for data distribution
   - Useful for quick probability estimates

4. **Properties**
   - Sum of normal variables is normal
   - Linear combination of normal variables is normal
   - Independent of sample size (unlike t-distribution)

#### Standard Normal Distribution

The Standard Normal Distribution is a special case of the normal distribution where μ = 0 and σ = 1. It's often denoted as N(0,1) and serves as a reference distribution.

##### Properties
- Mean (μ) = 0
- Standard Deviation (σ) = 1
- PDF: f(z) = (1/√(2π)) * e^(-z²/2)
- Symmetric around zero
- Total area = 1

##### Z-Table Areas
- z = ±1: 68% of data
- z = ±2: 95% of data
- z = ±3: 99.7% of data

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

def plot_standard_normal():
    z = np.linspace(-4, 4, 100)
    pdf = norm.pdf(z, 0, 1)
    
    plt.figure(figsize=(10, 6))
    plt.plot(z, pdf, 'b-', label='PDF')
    
    # Shade regions
    colors = ['red', 'blue', 'green']
    alphas = [0.1, 0.1, 0.1]
    for i, (c, a) in enumerate(zip(colors, alphas), 1):
        mask = (z >= -i) & (z <= i)
        plt.fill_between(z[mask], pdf[mask], color=c, alpha=a,
                        label=f'±{i}σ ({norm.cdf(i)-norm.cdf(-i):.1%})')
    
    plt.title('Standard Normal Distribution')
    plt.xlabel('Z-Score')
    plt.ylabel('Probability Density')
    plt.grid(True)
    plt.legend()
    return plt

# Example usage
plot = plot_standard_normal()
plt.show()
```
![Standard Normal Distribution](/images/Standard_Normal_Distribution.png)

##### Common Z-Score Values
```python
# Key probability points
z_scores = {
    0.90: 1.28,  # 90% confidence
    0.95: 1.96,  # 95% confidence
    0.99: 2.58   # 99% confidence
}

# Example: Finding probabilities
def get_z_probability(z):
    return norm.cdf(z) - norm.cdf(-z)

print(f"P(-1 < Z < 1): {get_z_probability(1):.4f}")  # 0.6827
print(f"P(-2 < Z < 2): {get_z_probability(2):.4f}")  # 0.9545
print(f"P(-3 < Z < 3): {get_z_probability(3):.4f}")  # 0.9973
```

##### Applications

1. **Standardization**
   ```python
   def standardize(x, mu, sigma):
       return (x - mu) / sigma
   
# Example: Test scores
scores = [75, 82, 90, 68, 95]
mu = np.mean(scores)
sigma = np.std(scores)
z_scores = [standardize(x, mu, sigma) for x in scores]
```

2. **Hypothesis Testing**
   ```python
   def z_test(sample_mean, pop_mean, pop_std, n):
       z = (sample_mean - pop_mean) / (pop_std / np.sqrt(n))
       p_value = 2 * (1 - norm.cdf(abs(z)))  # Two-tailed
       return z, p_value
   ```

3. **Confidence Intervals**
   ```python
   def confidence_interval(mean, std, n, confidence=0.95):
       z = norm.ppf((1 + confidence) / 2)
       margin = z * (std / np.sqrt(n))
       return mean - margin, mean + margin
   ```

##### Key Uses
1. Reference for normalizing data
2. Base for statistical inference
3. Quality control limits
4. Risk assessment
5. Hypothesis testing

##### Transformation
To convert between normal distributions:
- From N(μ,σ) to N(0,1): Z = (X - μ) / σ
- From N(0,1) to N(μ,σ): X = Zσ + μ

```python
# Example: Converting between distributions
def transform_distribution(x, from_params, to_params):
    """
    Transform value between normal distributions
    from_params: tuple of (mean, std) of original distribution
    to_params: tuple of (mean, std) of target distribution
    """
    from_mean, from_std = from_params
    to_mean, to_std = to_params
    
    # First standardize
    z = (x - from_mean) / from_std
    # Then transform to new distribution
    return z * to_std + to_mean

# Example usage
x = 85  # Score from N(75, 10)
new_x = transform_distribution(x, (75, 10), (100, 15))
print(f"Score of {x} transforms to {new_x:.1f}")
// Score of 85 transforms to 115.0
```

#### Uniform Distribution

The Uniform Distribution is a probability distribution where all outcomes in a given interval are equally likely to occur. It comes in two forms: discrete and continuous.

##### Types of Uniform Distributions

1. **Continuous Uniform Distribution**
   - Defined over continuous interval [a,b]
   - Also called "rectangular distribution"
   - Every point in interval has equal probability density

2. **Discrete Uniform Distribution**
   - Defined over finite set of equally spaced values
   - Each value has equal probability
   - Example: Fair die (values 1-6)

##### Mathematical Properties

1. **Continuous Uniform**
   - **PDF**: f(x) = 1/(b-a) for a ≤ x ≤ b, 0 otherwise
   - **CDF**: F(x) = (x-a)/(b-a) for a ≤ x ≤ b
   - **Mean**: μ = (a + b)/2
   - **Variance**: σ² = (b - a)²/12
   - **Median**: (a + b)/2
   - **Mode**: Any value in [a,b]
   - **Skewness**: 0 (symmetric)
   - **Kurtosis**: 9/5 (platykurtic)

2. **Discrete Uniform**
   - **PMF**: P(X = x) = 1/n for each x in {x₁, ..., xₙ}
   - **Mean**: (x₁ + xₙ)/2
   - **Variance**: (n² - 1)/12 where n is number of values

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import uniform

class UniformDistribution:
    def __init__(self, a=0, b=1):
        self.a = a
        self.b = b
        self.mean = (a + b) / 2
        self.variance = (b - a)**2 / 12
        self.std = np.sqrt(self.variance)
    
    def pdf(self, x):
        """Probability Density Function"""
        return np.where((x >= self.a) & (x <= self.b), 
                       1/(self.b - self.a), 0)
    
    def cdf(self, x):
        """Cumulative Distribution Function"""
        return np.clip((x - self.a)/(self.b - self.a), 0, 1)
    
    def simulate(self, n_samples):
        """Generate random samples"""
        return np.random.uniform(self.a, self.b, n_samples)

# Visualization of PDF and CDF
def plot_uniform_distribution(a=0, b=1):
    dist = UniformDistribution(a, b)
    x = np.linspace(a-0.5, b+0.5, 1000)
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
    
    # PDF
    ax1.plot(x, dist.pdf(x))
    ax1.fill_between(x, dist.pdf(x), alpha=0.3)
    ax1.set_title('Probability Density Function')
    ax1.grid(True)
    
    # CDF
    ax2.plot(x, dist.cdf(x))
    ax2.set_title('Cumulative Distribution Function')
    ax2.grid(True)
    
    plt.tight_layout()
    plt.show()

plot_uniform_distribution(0, 1)
```
![Uniform Distribution](/images/Uniform_Distribution.png)

##### Applications and Examples

1. **Random Number Generation**
```python
def random_number_generator(a, b, n=1):
    """Generate n random numbers between a and b"""
    uniform = UniformDistribution(a, b)
    return uniform.simulate(n)

# Generate 5 random numbers between 0 and 10
numbers = random_number_generator(0, 10, 5)
print(f"Random numbers: {numbers}")
```

2. **Simulation of Wait Times**
```python
class ServiceSimulator:
    def __init__(self, min_time=5, max_time=15):
        self.uniform = UniformDistribution(min_time, max_time)
    
    def simulate_service_times(self, n_customers):
        times = self.uniform.simulate(n_customers)
        return {
            'times': times,
            'average_wait': np.mean(times),
            'total_time': np.sum(times)
        }

# Simulate service times for 10 customers
simulator = ServiceSimulator()
results = simulator.simulate_service_times(10)
print(f"Average wait time: {results['average_wait']:.2f} minutes")
// Average wait time: 10.21 minutes
```

3. **Quality Control Bounds**
```python
class QualityControl:
    def __init__(self, target, tolerance):
        self.lower = target - tolerance
        self.upper = target + tolerance
        self.uniform = UniformDistribution(self.lower, self.upper)
    
    def check_production(self, n_items):
        measurements = self.uniform.simulate(n_items)
        in_spec = np.logical_and(
            measurements >= self.lower,
            measurements <= self.upper
        )
        return {
            'measurements': measurements,
            'pass_rate': np.mean(in_spec),
            'failures': np.sum(~in_spec)
        }

# Check 100 items with target 10 and tolerance ±0.5
qc = QualityControl(target=10, tolerance=0.5)
inspection = qc.check_production(100)
print(f"Pass rate: {inspection['pass_rate']:.1%}")
// Pass rate: 100.0%
```

##### Statistical Properties and Relationships

1. **Sum of Uniform Variables**
   - Sum tends toward normal distribution (CLT)
   - Special case: Irwin-Hall distribution

2. **Order Statistics**
   - Minimum: a + (b-a)U₁/₍ₙ₊₁₎
   - Maximum: a + (b-a)Uₙ/₍ₙ₊₁₎
   - Where U follows Beta distribution

```python
def demonstrate_sum_convergence(n_vars=12, n_samples=1000):
    """Demonstrate convergence to normal as we sum uniforms"""
    sums = np.sum([np.random.uniform(0, 1, n_samples) 
                   for _ in range(n_vars)], axis=0)
    
    plt.figure(figsize=(8, 4))
    plt.hist(sums, bins=30, density=True)
    plt.title(f'Sum of {n_vars} Uniform Variables')
    plt.xlabel('Sum')
    plt.ylabel('Density')
    return plt

demonstrate_sum_convergence()
```
![Sum of Uniform Variables](/images/Sum_of_Uniform_Variables.png)

##### Hypothesis Testing with Uniform Distribution

The uniform distribution is often used in:
1. Testing random number generators
2. Goodness-of-fit tests
3. P-value calculations

```python
def test_uniformity(data, alpha=0.05):
    """
    Kolmogorov-Smirnov test for uniformity
    """
    from scipy import stats
    
    ks_stat, p_value = stats.kstest(data, 'uniform')
    return {
        'statistic': ks_stat,
        'p_value': p_value,
        'uniform': p_value > alpha
    }

# Test random numbers for uniformity
data = np.random.uniform(0, 1, 1000)
results = test_uniformity(data)
print(f"Uniformity test p-value: {results['p_value']:.3f}")
// Uniformity test p-value: 0.999
```

##### Key Points and Best Practices

1. **When to Use**
   - Random sampling
   - Simple probability models
   - Null hypothesis testing
   - Initial approximations

2. **Limitations**
   - Assumes equal probability
   - May oversimplify real phenomena
   - Sensitive to interval bounds

3. **Common Mistakes**
   - Assuming uniformity without testing
   - Ignoring boundary effects
   - Misinterpreting discrete vs continuous

#### Log Normal Distribution

A Log Normal Distribution describes data where taking the natural log (ln) of the values gives a normal distribution. Think of it as a "skewed bell curve" that can't go below zero.

##### Why It's Important
- Models things that grow by percentage (like money or populations)
- Always positive (can't have negative values)
- Shows up naturally in many real-world situations

##### Key Features
- Skewed right (long tail on right side)
- Can't be negative
- Most values cluster near the left
- Has a few very large values on the right

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import lognorm

# Create simple example data
normal_data = np.random.normal(0, 0.5, 1000)
lognormal_data = np.exp(normal_data)  # Transform to lognormal

# Plot both to show relationship
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# Normal distribution plot
ax1.hist(normal_data, bins=30, density=True)
ax1.set_title('Normal Distribution')
ax1.set_xlabel('Value')

# Log normal distribution plot
ax2.hist(lognormal_data, bins=30, density=True)
ax2.set_title('Log Normal Distribution')
ax2.set_xlabel('Value')

plt.tight_layout()
plt.show()
```
![Log Normal Distribution](/images/Log_Normal_Distribution.png)

##### Real-World Examples

1. **House Prices**
```python
def simulate_house_prices(median_price=300000, spread=0.5, n_houses=1000):
    """Simulate house prices in a city"""
    mu = np.log(median_price)  # Convert median to log scale
    prices = np.random.lognormal(mu, spread, n_houses)
    
    return {
        'median': np.median(prices),
        'mean': np.mean(prices),
        'cheapest': np.min(prices),
        'most_expensive': np.max(prices)
    }

# Example
prices = simulate_house_prices()
print(f"Median house: ${prices['median']:,.0f}")
print(f"Average house: ${prices['mean']:,.0f}")
print(f"Price range: ${prices['cheapest']:,.0f} to ${prices['most_expensive']:,.0f}")
// Median house: $293,919
// Average house: $334,266
// Price range: $54,054 to $1,700,471
```

2. **Investment Growth**
```python
def investment_scenarios(initial=10000, years=30, risk=0.15):
    """Simulate possible investment outcomes"""
    annual_return = 0.07  # 7% average return
    
    # Generate 1000 possible scenarios
    final_amounts = initial * np.random.lognormal(
        (annual_return - risk**2/2) * years,
        risk * np.sqrt(years),
        1000
    )
    
    return {
        'median': np.median(final_amounts),
        'worst_10': np.percentile(final_amounts, 10),
        'best_10': np.percentile(final_amounts, 90)
    }

# Example
results = investment_scenarios()
print(f"Typical outcome: ${results['median']:,.0f}")
print(f"Range: ${results['worst_10']:,.0f} to ${results['best_10']:,.0f}")
// Typical outcome: $60,900
// Range: $19,375 to $173,670
```

##### When to Use It
Use log normal when your data:
1. Can't be negative (like prices or sizes)
2. Is skewed right (has a long tail to the right)
3. Grows by percentages rather than fixed amounts

##### Simple Rules of Thumb
- Most values will be below the mean
- Median is less than mean
- A few very large values will pull the mean up
- Multiplying/dividing by a constant shifts the distribution

##### Common Mistakes to Avoid
1. Using it for negative values (impossible)
2. Expecting symmetry (it's always skewed)
3. Using regular averages (use geometric mean instead)
4. Forgetting to transform back from log scale

```python
# Example showing common statistics
def log_normal_stats(data):
    """Calculate key statistics for log-normal data"""
    log_data = np.log(data)
    
    return {
        'median': np.exp(np.mean(log_data)),  # Geometric mean
        'mean': np.mean(data),                # Arithmetic mean
        'typical_range': [
            np.exp(np.mean(log_data) - np.std(log_data)),
            np.exp(np.mean(log_data) + np.std(log_data))
        ]
    }

# Example with salary data
salaries = np.random.lognormal(11, 0.5, 1000)  # Generate sample salaries
stats = log_normal_stats(salaries)

print(f"Typical salary (median): ${stats['median']:,.0f}")
print(f"Average salary (mean): ${stats['mean']:,.0f}")
print(f"Typical range: ${stats['typical_range'][0]:,.0f} to ${stats['typical_range'][1]:,.0f}")
// Typical salary (median): $31,623
// Average salary (mean): $34,859
// Typical range: $25,262 to $41,771
```

> **Remember**: Log normal distributions are perfect for things that grow by percentages (like money) or can't be negative (like sizes or times).



